2013년 구글 연구팀이 발표한 기법.
Efficient Estimation of Word Representations in Vector Space 논문 Mikolov 2013a
Distributed Representations of Words and Phrase and their Compositionality 논문 Mikolov 2013b

2013a는 Skip-Gram과 CBOW의 이야기
2013b는 2013a에 제시된 모델을 기반으로 학습 최적화 방법을 제시항 Negative Sampling 기법을 제시

CBOW는 주변 문맥 단어로 해당 단어를 유추
Skip-gram은 해당 단어로 주변 문맥 단어를 유추

Negative Sampling은 대상 단어와 주변 단어 쌍이 주어졌을 때 그 쌍이 Positive인지 Nagative인지 이진 분류로 학습하는 것.
횟수는 작업자가 조정 가능하며 그 횟수를 k로 쓰고 Positive 데이터까지 합쳐 총 k+1회 계산한다.
2013b에 따르면 negative sampling의 횟수는 작은 데이터 5~20, 큰 데이터 2~5로 놓고 계산하는 것이 성능이 좋았다고 함.

Negative 데이터를 뽑는 데 2개의 확률 수식이 사용됨.
1. Negative sample probability.  
.  P_negative(w_i)=frat{u(w_i)^3/4}{\sum\j=0\^n\U(w_j)^3/4}
2. subsampling
.  P_subsampling(w_i)=1-제곱근(frat{t}{f(w_t)})

1번은 자주 등장하지 않는 단어들에 대해 뽑힐 확률이 증가하는 것이고, 2번은 자주 등장하는 단어에 대해 확률을 낮춰주는 확률.

u_t×v_c는 두 벡터의 내적이며 동시에 코사인 유사도를 의미.

skip gram의 학습은 U,V 매트릭스 모두를 학습.. (word2vec 처음 볼 당시에는 몰랐음..)
결국 P(+|t,c) = frat{1}{1+exp(-u_t*v_c)}라는 수식이 positive 확률
위 수식을 키워야 함. exp(-u_t×v_c)값을 줄여야 함. → u_t×v_c값을 키워야 함

네거티브 단어에 대해서도 마찬가지. 네거티브 단어도 잘 맞춰야 함.
P(-|t,c) = 1-P(+|t,c) = frat{exp(-U_t×v_t)}{1+exp(-u_t×v_t)}
얘는 exp(-u_t×v_c)값을 키워야 확률이 올라감.
값을 키워야 하기 때문에 u_t×v_c 값을 낮춰야되고, 유사도는 멀어져간다.

결국 Skip gram은 다음 모델을 학습하는 것.
L(e)=logP(+|t_p,c_p)+/sum/_i=1/^k/logP(-|t_n,c_n)

Word2Vec은 대상단어에 대해 U매트릭스의 값이 출력 됨.
그러나 U말고도 U+V^T 또는 U,V^t concatenate 형태로 붙여쓸 수 있음.
